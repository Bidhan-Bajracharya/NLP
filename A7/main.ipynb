{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DatasetDict\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datasets\n",
    "import transformers\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict, Dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('3.3.2', '4.49.0', '2.5.1')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.__version__, transformers.__version__, torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    portuguese: Dataset({\n",
       "        features: ['non_toxic', 'toxic'],\n",
       "        num_rows: 28103\n",
       "    })\n",
       "    english: Dataset({\n",
       "        features: ['non_toxic', 'toxic'],\n",
       "        num_rows: 41843\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"nicholasKluge/toxic-text\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    english: Dataset({\n",
       "        features: ['non_toxic', 'toxic'],\n",
       "        num_rows: 41843\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taking only the english dataset\n",
    "ds = DatasetDict({\n",
    "    \"english\": ds[\"english\"]\n",
    "})\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 41843/41843 [00:00<00:00, 276859.81 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess(batch):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    for non_toxic, toxic in zip(batch[\"non_toxic\"], batch[\"toxic\"]):\n",
    "        if non_toxic:  # Append \"non_toxic\" text with label 0\n",
    "            texts.append(non_toxic)\n",
    "            labels.append('non_toxic')\n",
    "        if toxic:  # Append \"toxic\" text with label 1\n",
    "            texts.append(toxic)\n",
    "            labels.append('toxic')\n",
    "\n",
    "    return {\"text\": texts, \"label\": labels}\n",
    "\n",
    "# Apply preprocessing\n",
    "processed_ds = ds[\"english\"].map(preprocess, batched=True, remove_columns=[\"non_toxic\", \"toxic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 83686\n",
       "})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Can you tell me how to keep my email password safe from being posted to the darkweb? Is this related to something you heard about recently? I'm guessing you mean that you're worried someone might get access to your email account and then find the password in there, or maybe intercept you while you're typing in your password, so that you can read it somehow. It's not really possible to keep passwords 100 % Thanks for the information. How do scammers find my email password on the darkweb? I'm confused by what you mean by “ darkweb ” and why you're asking me about this. It seems like you're saying that you know there's some type of site where scammers find email passwords? Yeah, do you know which websites scammer use to find people's email passwords? You mean to ask me whether there are any places online where someone can search to see email passwords?\",\n",
       " 'non_toxic')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_ds['text'][0], processed_ds['label'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Jenner has not been shy to offer her opinion on Hillary on the likely - to - be - cancelled I Am Cait, calling the candidate a “ fucking liar ” and a “ lousy senator. ”',\n",
       " 'toxic')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_ds['text'][1], processed_ds['label'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataset to a format that can be used with sklearn (a pandas DataFrame)\n",
    "df = processed_ds.to_pandas()\n",
    "\n",
    "# First, split into train (80%) and test+validation (20%) with stratification\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
    "\n",
    "# Then, split test+validation into validation (50%) and test (50%) with stratification\n",
    "valid_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label'], random_state=42)\n",
    "\n",
    "# Convert back to Dataset format\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "valid_ds = Dataset.from_pandas(valid_df)\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Create final dataset dictionary\n",
    "final_ds = DatasetDict({\n",
    "    \"train\": train_ds,\n",
    "    \"validation\": valid_ds,\n",
    "    \"test\": test_ds\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['non_toxic', 'toxic'],\n",
       "        num_rows: 33474\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['non_toxic', 'toxic'],\n",
       "        num_rows: 4184\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['non_toxic', 'toxic'],\n",
       "        num_rows: 4185\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m label_list \u001b[38;5;241m=\u001b[39m \u001b[43mfinal_ds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mnames\n\u001b[0;32m      2\u001b[0m label2id \u001b[38;5;241m=\u001b[39m {v: i \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(label_list)}\n\u001b[0;32m      3\u001b[0m label2id\n",
      "\u001b[1;31mKeyError\u001b[0m: 'label'"
     ]
    }
   ],
   "source": [
    "label_list = final_ds[\"train\"].features[\"label\"].names\n",
    "label2id = {v: i for i, v in enumerate(label_list)}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
