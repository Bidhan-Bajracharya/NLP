{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Bidhan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from nltk.corpus import brown\n",
    "from collections import Counter\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choosing news as suggested\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_corpus = brown.sents(categories='news')\n",
    "news_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_corpus = news_corpus[:300] # taking small subset for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6642"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "news_flatten = flatten(news_corpus)\n",
    "len(news_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['needed', \"Daniel's\", 'outmoded', 'process', 'experienced']"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = list(set(news_flatten))\n",
    "vocab.append('<UNK>')\n",
    "vocab[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {w:i for (i, w) in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word = {v:k for k, v in word2index.items()} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1939"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(batch_size, corpus, window_size=2):\n",
    "    # Make skip gram of custom size window\n",
    "    skip_grams = []\n",
    "\n",
    "    for sent in corpus:\n",
    "        for target_index in range(window_size, len(sent) - window_size):\n",
    "            target = word2index[sent[target_index]]\n",
    "            context = []\n",
    "            count = window_size # count of context words to pick on the left and right\n",
    "            while count > 0:\n",
    "                # for default window, it will get the left most and right most word\n",
    "                # then the second left most and second right most word\n",
    "                context.append(word2index[sent[target_index - count]])\n",
    "                context.append(word2index[sent[target_index + count]])\n",
    "                count -= 1\n",
    "\n",
    "            for word in context:\n",
    "                skip_grams.append([target, word])\n",
    "    \n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_index = np.random.choice(range(len(skip_grams)), batch_size, replace=False) #randomly pick without replacement\n",
    "        \n",
    "    for i in random_index:\n",
    "        random_inputs.append([skip_grams[i][0]])  # target, e.g., 2\n",
    "        random_labels.append([skip_grams[i][1]])  # context word, e.g., 3\n",
    "            \n",
    "    return np.array(random_inputs), np.array(random_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  [[1081]\n",
      " [ 552]]\n",
      "Target:  [[ 156]\n",
      " [1291]]\n"
     ]
    }
   ],
   "source": [
    "#testing the method\n",
    "batch_size = 2 # mini-batch size\n",
    "input_batch, target_batch = random_batch(batch_size, news_corpus)\n",
    "\n",
    "print(\"Input: \", input_batch)\n",
    "print(\"Target: \", target_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skipgram(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(Skipgram, self).__init__()\n",
    "\n",
    "        self.embedding_center   = nn.Embedding(vocab_size, emb_size)\n",
    "        self.embedding_outside  = nn.Embedding(vocab_size, emb_size)\n",
    "\n",
    "    def forward(self, center, outside, all_vocab):\n",
    "        center_embedding    = self.embedding_center(center)     # as seen in above example, size: (batch_size, 1, embedding_size)\n",
    "        outside_embedding   = self.embedding_outside(outside)   # (batch_size, 1, embedding_size)\n",
    "        all_vocab_embedding = self.embedding_outside(all_vocab) # (batch_size, vocab_size, embedding_size)\n",
    "\n",
    "        numerator   = torch.exp(outside_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2))\n",
    "        # (b_size, 1, emb_size) @ (b_size, emb_size, 1) = (b_size, 1, 1) -> (b_size, 1)\n",
    "\n",
    "\n",
    "        denominator = all_vocab_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2)\n",
    "        # (b_size, vocab_size, emb_size) @ (b_size, emb_size, 1) = (b_size, vocab_size, 1) -> (b_size, vocab_size)\n",
    "\n",
    "        denominator_sum = torch.sum(torch.exp(denominator), 1)\n",
    "\n",
    "        loss = -torch.mean(torch.log(numerator / denominator_sum)) # scalar\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size     = 2 # mini-batch size\n",
    "embedding_size = 2\n",
    "model          = Skipgram(vocab_size, embedding_size)\n",
    "model_skipgram = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(w):\n",
    "    if word2index.get(w) is not None:\n",
    "        return word2index[w]\n",
    "    else:\n",
    "        return word2index['<UNK>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1939])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "#use for the normalized term in the probability calculation\n",
    "all_vocabs = prepare_sequence(list(vocab), word2index).expand(batch_size, len(vocab))  # [batch_size, voc_size]\n",
    "all_vocabs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 | cost: 7.214520 | time: 0m 0s\n",
      "Epoch: 2000 | cost: 7.755567 | time: 0m 0s\n",
      "Epoch: 3000 | cost: 6.652462 | time: 0m 0s\n",
      "Epoch: 4000 | cost: 7.302613 | time: 0m 0s\n",
      "Epoch: 5000 | cost: 7.604928 | time: 0m 0s\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "num_epochs = 5000\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    input_batch, target_batch = random_batch(batch_size, news_corpus)\n",
    "    input_batch  = torch.LongTensor(input_batch)  #[batch_size, 1]\n",
    "    target_batch = torch.LongTensor(target_batch) #[batch_size, 1]\n",
    "\n",
    "    # changing to cuda\n",
    "    input_batch  = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    all_vocabs   = all_vocabs.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = model_skipgram(input_batch, target_batch, all_vocabs)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f\"Epoch: {epoch + 1} | cost: {loss:.6f} | time: {epoch_mins}m {epoch_secs}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model_skipgram.state_dict(), 'model/skipgram_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model_skipgram, open('model/skipgram.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = vocab[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#numericalization\n",
    "id = word2index[word]\n",
    "id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_tensor = torch.LongTensor([id])\n",
    "id_tensor = id_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4518, 0.2702]], device='cuda:0', grad_fn=<EmbeddingBackward0>),\n",
       " tensor([[ 0.0529, -0.0310]], device='cuda:0', grad_fn=<EmbeddingBackward0>))"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the embedding by averaging\n",
    "v_embed = model_skipgram.embedding_center(id_tensor)\n",
    "u_embed = model_skipgram.embedding_outside(id_tensor)\n",
    "\n",
    "v_embed, u_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1196, device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#average to get the word embedding\n",
    "word_embed = (v_embed + u_embed) / 2\n",
    "word_embed[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed_skip_gram(word):\n",
    "    id_tensor = torch.LongTensor([word2index[word]])\n",
    "    id_tensor = id_tensor.to(device)\n",
    "    v_embed = model_skipgram.embedding_center(id_tensor)\n",
    "    u_embed = model_skipgram.embedding_outside(id_tensor) \n",
    "    word_embed = (v_embed + u_embed) / 2 \n",
    "    x, y = word_embed[0][0].item(), word_embed[0][1].item()\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(a, b):\n",
    "    cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "government = get_embed_skip_gram('government')\n",
    "officials = get_embed_skip_gram('officials')\n",
    "administration = get_embed_skip_gram('administration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "government vs officials: 0.7794\n",
      "government vs administration: -0.9817\n",
      "government vs government: 1.0000\n"
     ]
    }
   ],
   "source": [
    "print(f\"government vs officials: {cos_sim(government, officials):.4f}\")\n",
    "print(f\"government vs administration: {cos_sim(government, administration):.4f}\")\n",
    "print(f\"government vs government: {cos_sim(government, government):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skipgram (Negative Sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = Counter(news_flatten)\n",
    "num_total_words = sum([c for w, c in word_count.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6642"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_table = []\n",
    "\n",
    "for vo in vocab:\n",
    "    uw = word_count[vo] / num_total_words\n",
    "    uw_alpha = int((uw ** 0.75) / Z)\n",
    "    unigram_table.extend([vo] * uw_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 113,\n",
       "         ',': 93,\n",
       "         '.': 87,\n",
       "         'of': 77,\n",
       "         'to': 64,\n",
       "         'a': 53,\n",
       "         'and': 45,\n",
       "         'in': 45,\n",
       "         \"''\": 36,\n",
       "         '``': 36,\n",
       "         'for': 36,\n",
       "         'The': 31,\n",
       "         'would': 27,\n",
       "         'said': 27,\n",
       "         'by': 25,\n",
       "         'that': 25,\n",
       "         'was': 22,\n",
       "         'on': 22,\n",
       "         'be': 22,\n",
       "         'is': 19,\n",
       "         'as': 17,\n",
       "         'it': 15,\n",
       "         'he': 15,\n",
       "         'will': 15,\n",
       "         'which': 15,\n",
       "         'his': 14,\n",
       "         'at': 13,\n",
       "         'who': 13,\n",
       "         'Texas': 13,\n",
       "         'jury': 13,\n",
       "         'year': 13,\n",
       "         'an': 12,\n",
       "         '--': 12,\n",
       "         'not': 12,\n",
       "         'election': 11,\n",
       "         'bill': 11,\n",
       "         'with': 11,\n",
       "         'has': 11,\n",
       "         'from': 11,\n",
       "         'million': 11,\n",
       "         'House': 11,\n",
       "         'plan': 11,\n",
       "         'this': 10,\n",
       "         'are': 10,\n",
       "         'schools': 10,\n",
       "         'medical': 10,\n",
       "         'or': 10,\n",
       "         'Dallas': 9,\n",
       "         'pay': 9,\n",
       "         'County': 9,\n",
       "         'Fulton': 9,\n",
       "         'other': 9,\n",
       "         'President': 9,\n",
       "         'been': 9,\n",
       "         'He': 9,\n",
       "         'State': 9,\n",
       "         'It': 9,\n",
       "         'cases': 8,\n",
       "         'program': 8,\n",
       "         'grants': 8,\n",
       "         'school': 8,\n",
       "         'care': 8,\n",
       "         'state': 8,\n",
       "         'under': 8,\n",
       "         'had': 8,\n",
       "         'also': 8,\n",
       "         'first': 8,\n",
       "         'new': 8,\n",
       "         'up': 8,\n",
       "         'have': 8,\n",
       "         'more': 8,\n",
       "         'federal': 8,\n",
       "         'there': 8,\n",
       "         'bonds': 7,\n",
       "         'were': 7,\n",
       "         'passed': 7,\n",
       "         'tax': 7,\n",
       "         'court': 7,\n",
       "         'committee': 7,\n",
       "         'A': 7,\n",
       "         'some': 7,\n",
       "         'dollars': 7,\n",
       "         'per': 7,\n",
       "         '(': 7,\n",
       "         'one': 7,\n",
       "         'resolution': 7,\n",
       "         'made': 7,\n",
       "         'last': 7,\n",
       "         'out': 7,\n",
       "         '1': 7,\n",
       "         'home': 7,\n",
       "         'county': 7,\n",
       "         'funds': 7,\n",
       "         'its': 7,\n",
       "         ')': 7,\n",
       "         'should': 6,\n",
       "         'these': 6,\n",
       "         'increase': 6,\n",
       "         'them': 6,\n",
       "         'health': 6,\n",
       "         'ADC': 6,\n",
       "         'each': 6,\n",
       "         'Sen.': 6,\n",
       "         'Rep.': 6,\n",
       "         'Department': 6,\n",
       "         'In': 5,\n",
       "         'make': 5,\n",
       "         'law': 5,\n",
       "         'such': 5,\n",
       "         'proposed': 5,\n",
       "         'Wexler': 5,\n",
       "         'dental': 5,\n",
       "         'take': 5,\n",
       "         'This': 5,\n",
       "         'College': 5,\n",
       "         'property': 5,\n",
       "         'told': 5,\n",
       "         'persons': 5,\n",
       "         'Clark': 5,\n",
       "         'public': 5,\n",
       "         'race': 5,\n",
       "         'bills': 5,\n",
       "         'trial': 5,\n",
       "         'expected': 5,\n",
       "         '10': 5,\n",
       "         'they': 5,\n",
       "         'days': 5,\n",
       "         'security': 5,\n",
       "         'vote': 5,\n",
       "         'cent': 5,\n",
       "         'asked': 5,\n",
       "         'day': 5,\n",
       "         'go': 5,\n",
       "         'work': 5,\n",
       "         'Congress': 5,\n",
       "         'legislators': 5,\n",
       "         'ward': 5,\n",
       "         'Legislature': 5,\n",
       "         \"'\": 5,\n",
       "         'proposal': 5,\n",
       "         'payroll': 5,\n",
       "         'grand': 5,\n",
       "         'Dr.': 5,\n",
       "         'than': 5,\n",
       "         'city': 5,\n",
       "         'research': 5,\n",
       "         'now': 5,\n",
       "         'people': 5,\n",
       "         'their': 5,\n",
       "         'Monday': 5,\n",
       "         'Karns': 5,\n",
       "         'Senate': 5,\n",
       "         'Austin': 5,\n",
       "         'over': 5,\n",
       "         'Kennedy': 5,\n",
       "         'two': 5,\n",
       "         'but': 5,\n",
       "         'when': 5,\n",
       "         'did': 5,\n",
       "         'cost': 5,\n",
       "         'no': 5,\n",
       "         'years': 5,\n",
       "         'general': 5,\n",
       "         'social': 5,\n",
       "         'I': 5,\n",
       "         'night': 4,\n",
       "         'like': 4,\n",
       "         'involved': 4,\n",
       "         'most': 4,\n",
       "         'report': 4,\n",
       "         'precinct': 4,\n",
       "         'Judge': 4,\n",
       "         'Committee': 4,\n",
       "         'approved': 4,\n",
       "         'Republicans': 4,\n",
       "         'all': 4,\n",
       "         'teacher': 4,\n",
       "         'case': 4,\n",
       "         ':': 4,\n",
       "         'candidate': 4,\n",
       "         'time': 4,\n",
       "         'provide': 4,\n",
       "         'Education': 4,\n",
       "         'nursing': 4,\n",
       "         'taxes': 4,\n",
       "         'Parkhouse': 4,\n",
       "         'Williams': 4,\n",
       "         'City': 4,\n",
       "         'hospital': 4,\n",
       "         'statements': 4,\n",
       "         'voters': 4,\n",
       "         'Pelham': 4,\n",
       "         'given': 4,\n",
       "         'Washington': 4,\n",
       "         'help': 4,\n",
       "         'act': 4,\n",
       "         'Hartsfield': 4,\n",
       "         'B.': 4,\n",
       "         'being': 4,\n",
       "         'rural': 4,\n",
       "         'Highway': 4,\n",
       "         'Thursday': 4,\n",
       "         'recommended': 4,\n",
       "         'received': 4,\n",
       "         'campaign': 4,\n",
       "         'defendants': 4,\n",
       "         'after': 4,\n",
       "         'costs': 4,\n",
       "         'special': 4,\n",
       "         'against': 4,\n",
       "         'Atlanta': 4,\n",
       "         'deaf': 4,\n",
       "         'issue': 4,\n",
       "         'counties': 4,\n",
       "         'Bellows': 4,\n",
       "         'aged': 4,\n",
       "         'J.': 4,\n",
       "         'since': 4,\n",
       "         'water': 4,\n",
       "         'Ratcliff': 3,\n",
       "         'give': 3,\n",
       "         'courses': 3,\n",
       "         'construction': 3,\n",
       "         'dollar': 3,\n",
       "         'still': 3,\n",
       "         'voted': 3,\n",
       "         '24': 3,\n",
       "         'similar': 3,\n",
       "         'medicine': 3,\n",
       "         'political': 3,\n",
       "         'governor': 3,\n",
       "         '3': 3,\n",
       "         'high': 3,\n",
       "         'employment': 3,\n",
       "         'discrimination': 3,\n",
       "         'votes': 3,\n",
       "         'if': 3,\n",
       "         'could': 3,\n",
       "         'force': 3,\n",
       "         'But': 3,\n",
       "         'Criminal': 3,\n",
       "         'need': 3,\n",
       "         'These': 3,\n",
       "         'ballot': 3,\n",
       "         'annual': 3,\n",
       "         'billion': 3,\n",
       "         'got': 3,\n",
       "         'estimated': 3,\n",
       "         'Barber': 3,\n",
       "         'upon': 3,\n",
       "         'Vandiver': 3,\n",
       "         'Roberts': 3,\n",
       "         'none': 3,\n",
       "         'designed': 3,\n",
       "         'worth': 3,\n",
       "         'ones': 3,\n",
       "         'about': 3,\n",
       "         'highway': 3,\n",
       "         'local': 3,\n",
       "         'spent': 3,\n",
       "         'chairman': 3,\n",
       "         'Cook': 3,\n",
       "         'must': 3,\n",
       "         'Gov.': 3,\n",
       "         'elected': 3,\n",
       "         'message': 3,\n",
       "         'between': 3,\n",
       "         'Hospital': 3,\n",
       "         'authority': 3,\n",
       "         'hearing': 3,\n",
       "         'through': 3,\n",
       "         'both': 3,\n",
       "         'James': 3,\n",
       "         'Houston': 3,\n",
       "         'anonymous': 3,\n",
       "         'services': 3,\n",
       "         'Davis': 3,\n",
       "         'added': 3,\n",
       "         'any': 3,\n",
       "         'fair': 3,\n",
       "         'children': 3,\n",
       "         'future': 3,\n",
       "         'establishment': 3,\n",
       "         'Jr.': 3,\n",
       "         'seven': 3,\n",
       "         'action': 3,\n",
       "         'money': 3,\n",
       "         'degree': 3,\n",
       "         'recommendations': 3,\n",
       "         'homes': 3,\n",
       "         'scheduled': 3,\n",
       "         'administration': 3,\n",
       "         'very': 3,\n",
       "         'enabling': 3,\n",
       "         'family': 3,\n",
       "         'amendment': 3,\n",
       "         'age': 3,\n",
       "         'Jan.': 3,\n",
       "         'means': 3,\n",
       "         '4': 3,\n",
       "         'bankers': 3,\n",
       "         'whether': 3,\n",
       "         'proposals': 3,\n",
       "         'George': 3,\n",
       "         'superintendent': 3,\n",
       "         'taken': 3,\n",
       "         'may': 3,\n",
       "         'Sam': 3,\n",
       "         'constitutional': 3,\n",
       "         'reduce': 3,\n",
       "         'can': 3,\n",
       "         'into': 3,\n",
       "         'irregularities': 3,\n",
       "         'former': 3,\n",
       "         \"Georgia's\": 3,\n",
       "         'evidence': 3,\n",
       "         'E.': 3,\n",
       "         'Executive': 3,\n",
       "         'retirement': 3,\n",
       "         'banks': 3,\n",
       "         'address': 3,\n",
       "         'millions': 3,\n",
       "         'bond': 3,\n",
       "         'Fort': 3,\n",
       "         'citizens': 3,\n",
       "         '13': 3,\n",
       "         'Paris': 3,\n",
       "         'child': 3,\n",
       "         'precincts': 3,\n",
       "         'because': 3,\n",
       "         'soon': 3,\n",
       "         'polls': 3,\n",
       "         'states': 3,\n",
       "         'present': 3,\n",
       "         'practices': 3,\n",
       "         'Parsons': 3,\n",
       "         '65': 3,\n",
       "         \"mayor's\": 3,\n",
       "         'Daniel': 3,\n",
       "         'place': 3,\n",
       "         'teaching': 3,\n",
       "         'railroad': 3,\n",
       "         'petition': 3,\n",
       "         'before': 3,\n",
       "         'companies': 3,\n",
       "         'fight': 3,\n",
       "         'calls': 3,\n",
       "         'primary': 3,\n",
       "         'cities': 3,\n",
       "         'Friday': 3,\n",
       "         'career': 3,\n",
       "         'yesterday': 3,\n",
       "         'paid': 3,\n",
       "         'education': 3,\n",
       "         'hours': 3,\n",
       "         'Oklahoma': 3,\n",
       "         'receive': 3,\n",
       "         'School': 3,\n",
       "         'When': 3,\n",
       "         'much': 3,\n",
       "         'session': 3,\n",
       "         'amount': 3,\n",
       "         'White': 3,\n",
       "         'Bush': 3,\n",
       "         'college': 3,\n",
       "         'Berry': 3,\n",
       "         'meet': 3,\n",
       "         'needs': 3,\n",
       "         'Court': 3,\n",
       "         'attorney': 3,\n",
       "         'scholarships': 3,\n",
       "         'number': 3,\n",
       "         'District': 3,\n",
       "         'further': 3,\n",
       "         'charged': 3,\n",
       "         '2': 3,\n",
       "         'M.': 3,\n",
       "         'many': 3,\n",
       "         'Martin': 3,\n",
       "         'Charles': 3,\n",
       "         'attend': 3,\n",
       "         'doctor': 3,\n",
       "         'Grover': 3,\n",
       "         'term': 3,\n",
       "         'principal': 3,\n",
       "         'next': 3,\n",
       "         'get': 3,\n",
       "         'semester': 3,\n",
       "         'called': 3,\n",
       "         'patient': 3,\n",
       "         'those': 3,\n",
       "         '20': 3,\n",
       "         'investigation': 3,\n",
       "         'His': 3,\n",
       "         'insurance': 3,\n",
       "         'members': 3,\n",
       "         'welfare': 3,\n",
       "         'argued': 3,\n",
       "         'later': 3,\n",
       "         'Cotten': 3,\n",
       "         'causes': 3,\n",
       "         'constituted': 3,\n",
       "         'There': 3,\n",
       "         'long': 3,\n",
       "         'judges': 3,\n",
       "         '30': 3,\n",
       "         '23d': 3,\n",
       "         'programs': 3,\n",
       "         'system': 3,\n",
       "         'listed': 3,\n",
       "         'Georgia': 3,\n",
       "         'board': 3,\n",
       "         'our': 3,\n",
       "         'aid': 3,\n",
       "         'raises': 2,\n",
       "         'Louis': 2,\n",
       "         'populous': 2,\n",
       "         'required': 2,\n",
       "         'Hays': 2,\n",
       "         'something': 2,\n",
       "         'A.': 2,\n",
       "         'Two': 2,\n",
       "         'held': 2,\n",
       "         'pipeline': 2,\n",
       "         'mayor': 2,\n",
       "         'legislator': 2,\n",
       "         'Miller': 2,\n",
       "         'voting': 2,\n",
       "         'Science': 2,\n",
       "         'crisis': 2,\n",
       "         'Kan.': 2,\n",
       "         'scholarship': 2,\n",
       "         'personnel': 2,\n",
       "         'Chapman': 2,\n",
       "         'violation': 2,\n",
       "         'aside': 2,\n",
       "         'telephone': 2,\n",
       "         '58th': 2,\n",
       "         'districts': 2,\n",
       "         'priority': 2,\n",
       "         'suit': 2,\n",
       "         '$10': 2,\n",
       "         'bank': 2,\n",
       "         'Aug.': 2,\n",
       "         'what': 2,\n",
       "         'Paso': 2,\n",
       "         'order': 2,\n",
       "         'Sherman': 2,\n",
       "         'certificate': 2,\n",
       "         'ask': 2,\n",
       "         'Calls': 2,\n",
       "         'Tuesday': 2,\n",
       "         '$1,000': 2,\n",
       "         '4-year': 2,\n",
       "         'audience': 2,\n",
       "         'combined': 2,\n",
       "         'March': 2,\n",
       "         'Officials': 2,\n",
       "         'except': 2,\n",
       "         'leading': 2,\n",
       "         'community': 2,\n",
       "         'studied': 2,\n",
       "         'impossible': 2,\n",
       "         'R.': 2,\n",
       "         'hear': 2,\n",
       "         'urged': 2,\n",
       "         'modest': 2,\n",
       "         'week': 2,\n",
       "         'obtain': 2,\n",
       "         'Scott': 2,\n",
       "         'veteran': 2,\n",
       "         '6': 2,\n",
       "         'You': 2,\n",
       "         'chief': 2,\n",
       "         'previously': 2,\n",
       "         'involving': 2,\n",
       "         'greater': 2,\n",
       "         'sought': 2,\n",
       "         'Health': 2,\n",
       "         'choice': 2,\n",
       "         '8': 2,\n",
       "         'problems': 2,\n",
       "         'client': 2,\n",
       "         '1961-62': 2,\n",
       "         'three': 2,\n",
       "         'does': 2,\n",
       "         'cannot': 2,\n",
       "         'making': 2,\n",
       "         'representatives': 2,\n",
       "         'grant': 2,\n",
       "         'San': 2,\n",
       "         'escheat': 2,\n",
       "         'reported': 2,\n",
       "         'rescind': 2,\n",
       "         'speaker': 2,\n",
       "         'addition': 2,\n",
       "         'poll': 2,\n",
       "         '1963': 2,\n",
       "         'opposed': 2,\n",
       "         'requirement': 2,\n",
       "         'portion': 2,\n",
       "         '$37': 2,\n",
       "         'apparently': 2,\n",
       "         'Ridge': 2,\n",
       "         'employed': 2,\n",
       "         'American': 2,\n",
       "         'heard': 2,\n",
       "         'manner': 2,\n",
       "         'real': 2,\n",
       "         'hospitals': 2,\n",
       "         'brokers': 2,\n",
       "         'Authority': 2,\n",
       "         'boost': 2,\n",
       "         'student': 2,\n",
       "         'places': 2,\n",
       "         'trouble': 2,\n",
       "         'Chairman': 2,\n",
       "         'prejudicial': 2,\n",
       "         'illness': 2,\n",
       "         'cover': 2,\n",
       "         'Republican': 2,\n",
       "         'Jones': 2,\n",
       "         'finally': 2,\n",
       "         'methods': 2,\n",
       "         'lacking': 2,\n",
       "         'additional': 2,\n",
       "         'disclosure': 2,\n",
       "         'Denton': 2,\n",
       "         'study': 2,\n",
       "         'Colquitt': 2,\n",
       "         'another': 2,\n",
       "         'revolving': 2,\n",
       "         'less': 2,\n",
       "         'sponsor': 2,\n",
       "         'Joe': 2,\n",
       "         'sponsored': 2,\n",
       "         'expense': 2,\n",
       "         'April': 2,\n",
       "         'debate': 2,\n",
       "         'advisement': 2,\n",
       "         'Ivan': 2,\n",
       "         'providing': 2,\n",
       "         'appointment': 2,\n",
       "         'today': 2,\n",
       "         'served': 2,\n",
       "         'Ordinary': 2,\n",
       "         'large': 2,\n",
       "         'including': 2,\n",
       "         'Worth': 2,\n",
       "         'plans': 2,\n",
       "         'teachers': 2,\n",
       "         'big': 2,\n",
       "         'Griffin': 2,\n",
       "         'back': 2,\n",
       "         'four': 2,\n",
       "         'University': 2,\n",
       "         'measure': 2,\n",
       "         'details': 2,\n",
       "         'government': 2,\n",
       "         'jurors': 2,\n",
       "         'contracts': 2,\n",
       "         'unit': 2,\n",
       "         '$1,500': 2,\n",
       "         'approve': 2,\n",
       "         'Paradise': 2,\n",
       "         'enter': 2,\n",
       "         'so': 2,\n",
       "         'getting': 2,\n",
       "         \"didn't\": 2,\n",
       "         'budget': 2,\n",
       "         'Berlin': 2,\n",
       "         'Weatherford': 2,\n",
       "         'himself': 2,\n",
       "         'own': 2,\n",
       "         'yet': 2,\n",
       "         'Salinger': 2,\n",
       "         'El': 2,\n",
       "         'president': 2,\n",
       "         'commented': 2,\n",
       "         'replied': 2,\n",
       "         'ever': 2,\n",
       "         'offer': 2,\n",
       "         'among': 2,\n",
       "         'schooling': 2,\n",
       "         'costly': 2,\n",
       "         'exception': 2,\n",
       "         'director': 2,\n",
       "         \"Atlanta's\": 2,\n",
       "         'directed': 2,\n",
       "         'eight': 2,\n",
       "         'John': 2,\n",
       "         'date': 2,\n",
       "         'couple': 2,\n",
       "         \"President's\": 2,\n",
       "         'saw': 2,\n",
       "         'gas': 2,\n",
       "         'serious': 2,\n",
       "         'solve': 2,\n",
       "         'privilege': 2,\n",
       "         'After': 2,\n",
       "         'unspecified': 2,\n",
       "         'elaborate': 2,\n",
       "         'fund': 2,\n",
       "         'representing': 2,\n",
       "         'retailers': 2,\n",
       "         'matching': 2,\n",
       "         'meeting': 2,\n",
       "         'test': 2,\n",
       "         'personally': 2,\n",
       "         'Blue': 2,\n",
       "         'Allen': 2,\n",
       "         'took': 2,\n",
       "         'largest': 2,\n",
       "         'U.S.': 2,\n",
       "         'toward': 2,\n",
       "         'then': 2,\n",
       "         'airport': 2,\n",
       "         'operated': 2,\n",
       "         'defeated': 2,\n",
       "         'little': 2,\n",
       "         'adjournment': 2,\n",
       "         'Rural': 2,\n",
       "         'Felix': 2,\n",
       "         'Cox': 2,\n",
       "         'resigned': 2,\n",
       "         'several': 2,\n",
       "         'person': 2,\n",
       "         'require': 2,\n",
       "         'Bill': 2,\n",
       "         'effort': 2,\n",
       "         'says': 2,\n",
       "         'confidence': 2,\n",
       "         'facilities': 2,\n",
       "         'we': 2,\n",
       "         'generally': 2,\n",
       "         'prison': 2,\n",
       "         'orderly': 2,\n",
       "         'One': 2,\n",
       "         'W.': 2,\n",
       "         'build': 2,\n",
       "         'problem': 2,\n",
       "         'laws': 2,\n",
       "         'sign': 2,\n",
       "         'paying': 2,\n",
       "         'set': 2,\n",
       "         'interest': 2,\n",
       "         'C.': 2,\n",
       "         'however': 2,\n",
       "         'Association': 2,\n",
       "         'veiled': 2,\n",
       "         \"year's\": 2,\n",
       "         'Other': 2,\n",
       "         'recommend': 2,\n",
       "         'workers': 2,\n",
       "         'permit': 2,\n",
       "         'where': 2,\n",
       "         'obtained': 2,\n",
       "         'petitions': 2,\n",
       "         'follow': 2,\n",
       "         'legislation': 2,\n",
       "         \"Department's\": 2,\n",
       "         'approval': 2,\n",
       "         'students': 2,\n",
       "         'source': 2,\n",
       "         'succeeded': 2,\n",
       "         'Sept.': 2,\n",
       "         'end': 2,\n",
       "         'sp.': 2,\n",
       "         'stocks': 2,\n",
       "         'reports': 2,\n",
       "         'During': 2,\n",
       "         '&': 2,\n",
       "         'my': 2,\n",
       "         'Full': 2,\n",
       "         'Henry': 2,\n",
       "         'Leader': 2,\n",
       "         'benefits': 2,\n",
       "         'Under': 2,\n",
       "         'They': 2,\n",
       "         'Roads': 2,\n",
       "         'courts': 2,\n",
       "         'notice': 2,\n",
       "         'opposition': 2,\n",
       "         'Feb.': 2,\n",
       "         'drafts': 2,\n",
       "         'occupation': 2,\n",
       "         'Welfare': 2,\n",
       "         'eliminating': 2,\n",
       "         'do': 2,\n",
       "         'authorizing': 2,\n",
       "         'only': 2,\n",
       "         'credit': 2,\n",
       "         'Superior': 2,\n",
       "         'horse': 2,\n",
       "         'you': 2,\n",
       "         'earlier': 2,\n",
       "         'assistance': 2,\n",
       "         'betting': 2,\n",
       "         'load': 2,\n",
       "         \"taxpayers'\": 2,\n",
       "         'fine': 2,\n",
       "         'Caldwell': 2,\n",
       "         'guilt': 2,\n",
       "         'run': 2,\n",
       "         'five': 2,\n",
       "         'departments': 2,\n",
       "         'Mayor': 2,\n",
       "         'D.': 2,\n",
       "         'wife': 2,\n",
       "         'possible': 2,\n",
       "         'payment': 2,\n",
       "         'deputies': 2,\n",
       "         'While': 2,\n",
       "         'charge': 2,\n",
       "         'roads': 2,\n",
       "         'senator': 2,\n",
       "         'brought': 2,\n",
       "         'enough': 2,\n",
       "         'say': 2,\n",
       "         'went': 2,\n",
       "         'denied': 2,\n",
       "         'rejected': 2,\n",
       "         'candidates': 2,\n",
       "         'building': 2,\n",
       "         'William': 2,\n",
       "         'Halleck': 2,\n",
       "         'issued': 2,\n",
       "         'racial': 2,\n",
       "         'jail': 2,\n",
       "         'previous': 2,\n",
       "         '50': 2,\n",
       "         'carry': 2,\n",
       "         'Sunday': 2,\n",
       "         'fire': 2,\n",
       "         'better': 2,\n",
       "         'worked': 2,\n",
       "         'dependency': 2,\n",
       "         'use': 2,\n",
       "         '?': 2,\n",
       "         'coolest': 2,\n",
       "         'controversy': 2,\n",
       "         'savings': 2,\n",
       "         'Research': 2,\n",
       "         'bit': 2,\n",
       "         'needed': 1,\n",
       "         \"Daniel's\": 1,\n",
       "         'outmoded': 1,\n",
       "         'process': 1,\n",
       "         'experienced': 1,\n",
       "         'Tax': 1,\n",
       "         '12': 1,\n",
       "         'Gaynor': 1,\n",
       "         'watered': 1,\n",
       "         \"ordinary's\": 1,\n",
       "         'devote': 1,\n",
       "         'event': 1,\n",
       "         'Summerdale': 1,\n",
       "         'extern': 1,\n",
       "         'license': 1,\n",
       "         'birth': 1,\n",
       "         'Raymondville': 1,\n",
       "         'title': 1,\n",
       "         \"wasn't\": 1,\n",
       "         'signatures': 1,\n",
       "         'kept': 1,\n",
       "         'insure': 1,\n",
       "         '$4': 1,\n",
       "         'prosecution': 1,\n",
       "         'near': 1,\n",
       "         'Pye': 1,\n",
       "         'diagnostic': 1,\n",
       "         'viewed': 1,\n",
       "         'reservoirs': 1,\n",
       "         'based': 1,\n",
       "         'Affairs': 1,\n",
       "         'institute': 1,\n",
       "         'reconsideration': 1,\n",
       "         'Nursing': 1,\n",
       "         'following': 1,\n",
       "         'licensing': 1,\n",
       "         'effect': 1,\n",
       "         \"I'm\": 1,\n",
       "         'whereby': 1,\n",
       "         'Lawrence': 1,\n",
       "         'request': 1,\n",
       "         'proceedings': 1,\n",
       "         'two-thirds': 1,\n",
       "         'erase': 1,\n",
       "         '750': 1,\n",
       "         'visiting': 1,\n",
       "         'seemed': 1,\n",
       "         'underground': 1,\n",
       "         'entering': 1,\n",
       "         '92': 1,\n",
       "         'tomorrow': 1,\n",
       "         'Everything': 1,\n",
       "         'subjected': 1,\n",
       "         'sheriff': 1,\n",
       "         'Chester': 1,\n",
       "         'according': 1,\n",
       "         'business': 1,\n",
       "         'dentistry': 1,\n",
       "         'Snodgrass': 1,\n",
       "         'drop': 1,\n",
       "         'efficiency': 1,\n",
       "         'enthusiasm': 1,\n",
       "         'Mayor-nominate': 1,\n",
       "         'Both': 1,\n",
       "         'realize': 1,\n",
       "         'Clarence': 1,\n",
       "         'quickie': 1,\n",
       "         'Cost': 1,\n",
       "         'subpenas': 1,\n",
       "         'acts': 1,\n",
       "         'encouragement': 1,\n",
       "         'hard-fought': 1,\n",
       "         'permitted': 1,\n",
       "         'fee': 1,\n",
       "         'taxpayers': 1,\n",
       "         'monthly': 1,\n",
       "         'support': 1,\n",
       "         'Operating': 1,\n",
       "         'socialized': 1,\n",
       "         'led': 1,\n",
       "         'keynote': 1,\n",
       "         'propose': 1,\n",
       "         'underlying': 1,\n",
       "         'Instead': 1,\n",
       "         'praised': 1,\n",
       "         'taunted': 1,\n",
       "         'enlarged': 1,\n",
       "         'referendum': 1,\n",
       "         'lawyer': 1,\n",
       "         'backed': 1,\n",
       "         'Aikin': 1,\n",
       "         'figures': 1,\n",
       "         'Hudson': 1,\n",
       "         '47': 1,\n",
       "         'Island': 1,\n",
       "         'modernizing': 1,\n",
       "         'amicable': 1,\n",
       "         'weekend': 1,\n",
       "         'midnight': 1,\n",
       "         'major': 1,\n",
       "         'actually': 1,\n",
       "         'Without': 1,\n",
       "         'Army': 1,\n",
       "         'remedy': 1,\n",
       "         'Newton': 1,\n",
       "         'composition': 1,\n",
       "         'reduced': 1,\n",
       "         'innocence': 1,\n",
       "         'over-all': 1,\n",
       "         'sending': 1,\n",
       "         'Plainview': 1,\n",
       "         'calendar': 1,\n",
       "         'joined': 1,\n",
       "         'ambitious': 1,\n",
       "         'reaction': 1,\n",
       "         'official': 1,\n",
       "         'sounded': 1,\n",
       "         'Supt.': 1,\n",
       "         '22': 1,\n",
       "         'levy': 1,\n",
       "         'areas': 1,\n",
       "         'expects': 1,\n",
       "         'information': 1,\n",
       "         'lived': 1,\n",
       "         '114': 1,\n",
       "         'pricing': 1,\n",
       "         'past': 1,\n",
       "         'effected': 1,\n",
       "         'thru': 1,\n",
       "         'Aj': 1,\n",
       "         'amounts': 1,\n",
       "         'Garland': 1,\n",
       "         'officials': 1,\n",
       "         'Natural': 1,\n",
       "         \"Commissioner's\": 1,\n",
       "         'prediction': 1,\n",
       "         'Harris': 1,\n",
       "         'let': 1,\n",
       "         'together': 1,\n",
       "         'registration': 1,\n",
       "         'advocacy': 1,\n",
       "         'regents': 1,\n",
       "         'eliminate': 1,\n",
       "         'delegation': 1,\n",
       "         'policeman': 1,\n",
       "         'However': 1,\n",
       "         'stake': 1,\n",
       "         'chemistry': 1,\n",
       "         'Those': 1,\n",
       "         'preserving': 1,\n",
       "         'Doctor': 1,\n",
       "         'while': 1,\n",
       "         'heavily': 1,\n",
       "         'returned': 1,\n",
       "         '1913': 1,\n",
       "         'doubling': 1,\n",
       "         'Buchanan': 1,\n",
       "         'honor': 1,\n",
       "         'sales': 1,\n",
       "         '87-31': 1,\n",
       "         'development': 1,\n",
       "         'noted': 1,\n",
       "         'selected': 1,\n",
       "         'knowledge': 1,\n",
       "         'here': 1,\n",
       "         'whipped': 1,\n",
       "         'every': 1,\n",
       "         'item': 1,\n",
       "         'result': 1,\n",
       "         'professor': 1,\n",
       "         'Frank': 1,\n",
       "         'operating': 1,\n",
       "         'L.': 1,\n",
       "         'although': 1,\n",
       "         'Revenue': 1,\n",
       "         'bureau': 1,\n",
       "         'higher': 1,\n",
       "         '1923': 1,\n",
       "         'East': 1,\n",
       "         'parimutuels': 1,\n",
       "         'misuse': 1,\n",
       "         'Failure': 1,\n",
       "         'sent': 1,\n",
       "         'Claims': 1,\n",
       "         'unity': 1,\n",
       "         'Master': 1,\n",
       "         'council': 1,\n",
       "         'increased': 1,\n",
       "         'Schley': 1,\n",
       "         'emphasizing': 1,\n",
       "         'outright': 1,\n",
       "         'sway': 1,\n",
       "         'miscount': 1,\n",
       "         'man': 1,\n",
       "         'blue': 1,\n",
       "         'questioned': 1,\n",
       "         'fees': 1,\n",
       "         'absorbed': 1,\n",
       "         'correctness': 1,\n",
       "         'Before': 1,\n",
       "         'Contributions': 1,\n",
       "         'ruled': 1,\n",
       "         'death': 1,\n",
       "         'Tabb': 1,\n",
       "         'filed': 1,\n",
       "         'asking': 1,\n",
       "         'scholastic': 1,\n",
       "         'Faced': 1,\n",
       "         'opened': 1,\n",
       "         'dismissed': 1,\n",
       "         'Deaf': 1,\n",
       "         '100,000': 1,\n",
       "         'undermine': 1,\n",
       "         'totaling': 1,\n",
       "         'living': 1,\n",
       "         'smooth': 1,\n",
       "         'mammoth': 1,\n",
       "         \"wife's\": 1,\n",
       "         'July': 1,\n",
       "         'pending': 1,\n",
       "         'option': 1,\n",
       "         '4.4': 1,\n",
       "         'Policeman': 1,\n",
       "         'pockets': 1,\n",
       "         'storage': 1,\n",
       "         'shortly': 1,\n",
       "         'We': 1,\n",
       "         'automobile': 1,\n",
       "         'statement': 1,\n",
       "         'Wednesday': 1,\n",
       "         \"school's\": 1,\n",
       "         'Actually': 1,\n",
       "         'constructing': 1,\n",
       "         'outgoing': 1,\n",
       "         'Opponents': 1,\n",
       "         'malingering': 1,\n",
       "         'Wise': 1,\n",
       "         'church': 1,\n",
       "         'urban': 1,\n",
       "         'Jackson': 1,\n",
       "         'talked': 1,\n",
       "         'specifically': 1,\n",
       "         'accounts': 1,\n",
       "         'all-woman': 1,\n",
       "         'direct': 1,\n",
       "         'done': 1,\n",
       "         'dependent': 1,\n",
       "         'intends': 1,\n",
       "         'latest': 1,\n",
       "         'Outlays': 1,\n",
       "         'mighty': 1,\n",
       "         'quickly': 1,\n",
       "         \"Cotten's\": 1,\n",
       "         'wind': 1,\n",
       "         'reelection': 1,\n",
       "         'majorities': 1,\n",
       "         ...})"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(unigram_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sampling(targets, unigram_table, k):\n",
    "    batch_size = targets.shape[0]\n",
    "    neg_samples = []\n",
    "    for i in range(batch_size):  #(1, k)\n",
    "        target_index = targets[i].item()\n",
    "        nsample      = []\n",
    "        while (len(nsample) < k):\n",
    "            neg = random.choice(unigram_table)\n",
    "            if word2index[neg] == target_index:\n",
    "                continue\n",
    "            nsample.append(neg)\n",
    "        neg_samples.append(prepare_sequence(nsample, word2index).reshape(1, -1))\n",
    "        \n",
    "    return torch.cat(neg_samples) #batch_size, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "x, y = random_batch(batch_size, news_corpus)\n",
    "x_tensor = torch.LongTensor(x)\n",
    "y_tensor = torch.LongTensor(y)\n",
    "x_tensor = x_tensor.to(device)\n",
    "y_tensor = y_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "neg_samples = negative_sampling(y_tensor, unigram_table, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([463], device='cuda:0')"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 356, 1140,  688, 1520,  701])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipgramNeg(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(SkipgramNeg, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "        self.logsigmoid        = nn.LogSigmoid()\n",
    "    \n",
    "    def forward(self, center, outside, negative):\n",
    "        #center, outside:  (bs, 1)\n",
    "        #negative       :  (bs, k)\n",
    "        \n",
    "        center_embed   = self.embedding_center(center) #(bs, 1, emb_size)\n",
    "        outside_embed  = self.embedding_outside(outside) #(bs, 1, emb_size)\n",
    "        negative_embed = self.embedding_outside(negative) #(bs, k, emb_size)\n",
    "\n",
    "        \n",
    "        uovc           = outside_embed.bmm(center_embed.transpose(1, 2)).squeeze(2) #(bs, 1)\n",
    "        ukvc           = -negative_embed.bmm(center_embed.transpose(1, 2)).squeeze(2) #(bs, k)\n",
    "        ukvc_sum       = torch.sum(ukvc, 1).reshape(-1, 1) #(bs, 1)\n",
    "        \n",
    "        loss           = self.logsigmoid(uovc) + self.logsigmoid(ukvc_sum)\n",
    "        \n",
    "        return -torch.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 2\n",
    "voc_size = len(vocab)\n",
    "model = SkipgramNeg(voc_size, emb_size)\n",
    "model_neg = model.to(device)\n",
    "neg_samples = neg_samples.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7988, device='cuda:0', grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = model_neg(x_tensor, y_tensor, neg_samples)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model_neg.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1000 | Loss: 2.428288\n",
      "Epoch   2000 | Loss: 0.939606\n",
      "Epoch   3000 | Loss: 1.476746\n",
      "Epoch   4000 | Loss: 1.084400\n",
      "Epoch   5000 | Loss: 1.893235\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    #get batch\n",
    "    input_batch, label_batch = random_batch(batch_size, news_corpus)\n",
    "    input_tensor = torch.LongTensor(input_batch)\n",
    "    label_tensor = torch.LongTensor(label_batch)\n",
    "\n",
    "    #move to cuda\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    label_tensor = label_tensor.to(device)\n",
    "    \n",
    "    #predict\n",
    "    neg_samples = negative_sampling(label_tensor, unigram_table, k)\n",
    "    neg_samples = neg_samples.to(device)\n",
    "\n",
    "    loss = model_neg(input_tensor, label_tensor, neg_samples)\n",
    "    \n",
    "    #backprogate\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    #update alpha\n",
    "    optimizer.step()\n",
    "    \n",
    "    #print the loss\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f\"Epoch {epoch+1:6.0f} | Loss: {loss:2.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model_neg.state_dict(), 'model/skipgram_neg_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model_neg, open('model/skipgram_neg.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed_neg_sample(word):\n",
    "    id_tensor = torch.LongTensor([word2index[word]])\n",
    "    id_tensor = id_tensor.to(device)\n",
    "    v_embed = model_neg.embedding_center(id_tensor)\n",
    "    u_embed = model_neg.embedding_outside(id_tensor) \n",
    "    word_embed = (v_embed + u_embed) / 2 \n",
    "    x, y = word_embed[0][0].item(), word_embed[0][1].item()\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "government = get_embed_neg_sample('government')\n",
    "officials = get_embed_neg_sample('officials')\n",
    "administration = get_embed_neg_sample('administration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "government vs officials: -0.4719\n",
      "government vs administration: 0.7468\n",
      "government vs government: 1.0000\n"
     ]
    }
   ],
   "source": [
    "print(f\"government vs officials: {cos_sim(government, officials):.4f}\")\n",
    "print(f\"government vs administration: {cos_sim(government, administration):.4f}\")\n",
    "print(f\"government vs government: {cos_sim(government, government):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skipgram(window_size = 2):\n",
    "    # Make skip gram of custom size window\n",
    "    skip_grams = []\n",
    "\n",
    "    for sent in news_corpus:\n",
    "        for target_index in range(window_size, len(sent) - window_size):\n",
    "            target = sent[target_index]\n",
    "            context = []\n",
    "            count = window_size # count of context words to pick on the left and right\n",
    "            while count > 0:\n",
    "                # for default window, it will get the left most and right most word\n",
    "                # then the second left most and second right most word\n",
    "                context.append(sent[target_index - count])\n",
    "                context.append(sent[target_index + count])\n",
    "                count -= 1\n",
    "\n",
    "            for word in context:\n",
    "                skip_grams.append((target, word))\n",
    "    return skip_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('County', 'The'),\n",
       " ('County', 'Jury'),\n",
       " ('County', 'Fulton'),\n",
       " ('County', 'Grand'),\n",
       " ('Grand', 'Fulton'),\n",
       " ('Grand', 'said'),\n",
       " ('Grand', 'County'),\n",
       " ('Grand', 'Jury')]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_grams = get_skipgram(2)\n",
    "skip_grams[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ik_skipgram = Counter(skip_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simply a normalized function...don't worry too much\n",
    "def weighting(w_i, w_j, X_ik):\n",
    "        \n",
    "    #check whether the co-occurrences exist between these two words\n",
    "    try:\n",
    "        x_ij = X_ik[(w_i, w_j)]\n",
    "    except:\n",
    "        x_ij = 1  #if does not exist, set it to 1\n",
    "                \n",
    "    x_max = 100 #100 # fixed in paper  #cannot exceed 100 counts\n",
    "    alpha = 0.75\n",
    "    \n",
    "    #if co-occurrence does not exceed 100, scale it based on some alpha\n",
    "    if x_ij < x_max:\n",
    "        result = (x_ij/x_max)**alpha  #scale it\n",
    "    else:\n",
    "        result = 1  #if is greater than max, set it to 1 maximum\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement\n",
    "\n",
    "X_ik = {}  #for keeping the co-occurences\n",
    "weighting_dic = {} #scaling the percentage of sampling\n",
    "\n",
    "for bigram in combinations_with_replacement(vocab, 2):\n",
    "    if X_ik_skipgram.get(bigram) is not None:  #matches \n",
    "        co_occer = X_ik_skipgram[bigram]  #get the count from what we already counted\n",
    "        X_ik[bigram] = co_occer + 1 # + 1 for stability issue\n",
    "        X_ik[(bigram[1],bigram[0])] = co_occer+1   #count also for the opposite\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    weighting_dic[bigram] = weighting(bigram[0], bigram[1], X_ik)\n",
    "    weighting_dic[(bigram[1], bigram[0])] = weighting(bigram[1], bigram[0], X_ik)\n",
    "\n",
    "# print(f\"{X_ik=}\")\n",
    "# print(f\"{weighting_dic=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(batch_size, word_sequence, skip_grams, X_ik, weighting_dic):\n",
    "    \n",
    "    #convert to id since our skip_grams is word, not yet id\n",
    "    skip_grams_id = [(word2index[skip_gram[0]], word2index[skip_gram[1]]) for skip_gram in skip_grams]\n",
    "    \n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_coocs  = []\n",
    "    random_weightings = []\n",
    "    random_index = np.random.choice(range(len(skip_grams_id)), batch_size, replace=False) #randomly pick without replacement\n",
    "        \n",
    "    for i in random_index:\n",
    "        random_inputs.append([skip_grams_id[i][0]])  # target, e.g., 2\n",
    "        random_labels.append([skip_grams_id[i][1]])  # context word, e.g., 3\n",
    "        \n",
    "        #get cooc\n",
    "        pair = skip_grams[i]\n",
    "        try:\n",
    "            cooc = X_ik[pair]\n",
    "        except:\n",
    "            cooc = 1\n",
    "        random_coocs.append([math.log(cooc)])\n",
    "        \n",
    "        #get weighting\n",
    "        weighting = weighting_dic[pair]\n",
    "        random_weightings.append([weighting])\n",
    "                    \n",
    "    return np.array(random_inputs), np.array(random_labels), np.array(random_coocs), np.array(random_weightings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  [[317]\n",
      " [390]]\n",
      "Target:  [[1698]\n",
      " [1301]]\n",
      "Cooc:  [[1.38629436]\n",
      " [0.69314718]]\n",
      "Weighting:  [[0.08944272]\n",
      " [0.05318296]]\n"
     ]
    }
   ],
   "source": [
    "#testing the method\n",
    "batch_size = 2 # mini-batch size\n",
    "input_batch, target_batch, cooc_batch, weighting_batch = random_batch(batch_size, news_corpus, skip_grams, X_ik, weighting_dic)\n",
    "\n",
    "print(\"Input: \", input_batch)\n",
    "print(\"Target: \", target_batch)\n",
    "print(\"Cooc: \", cooc_batch)\n",
    "print(\"Weighting: \", weighting_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVe(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size,embed_size):\n",
    "        super(GloVe,self).__init__()\n",
    "        self.embedding_center = nn.Embedding(vocab_size, embed_size) # center embedding\n",
    "        self.embedding_outside = nn.Embedding(vocab_size, embed_size) # out embedding\n",
    "        \n",
    "        self.v_bias = nn.Embedding(vocab_size, 1)\n",
    "        self.u_bias = nn.Embedding(vocab_size, 1)\n",
    "        \n",
    "    def forward(self, center_words, target_words, coocs, weighting):\n",
    "        center_embeds = self.embedding_center(center_words) # [batch_size, 1, emb_size]\n",
    "        target_embeds = self.embedding_outside(target_words) # [batch_size, 1, emb_size]\n",
    "        \n",
    "        center_bias = self.v_bias(center_words).squeeze(1)\n",
    "        target_bias = self.u_bias(target_words).squeeze(1)\n",
    "        \n",
    "        inner_product = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        #[batch_size, 1, emb_size] @ [batch_size, emb_size, 1] = [batch_size, 1, 1] = [batch_size, 1]\n",
    "        \n",
    "        #note that coocs already got log\n",
    "        loss = weighting*torch.pow(inner_product +center_bias + target_bias - coocs, 2)\n",
    "        \n",
    "        return torch.sum(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size     = 10 # mini-batch size\n",
    "embedding_size = 2 #so we can later plot\n",
    "model_glove    = GloVe(voc_size, embedding_size)\n",
    "model_glove    = model_glove.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_glove.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 | cost: 13.994851 | time: 0m 0s\n",
      "Epoch: 2000 | cost: 9.683171 | time: 0m 0s\n",
      "Epoch: 3000 | cost: 0.930418 | time: 0m 0s\n",
      "Epoch: 4000 | cost: 1.735038 | time: 0m 0s\n",
      "Epoch: 5000 | cost: 2.337914 | time: 0m 0s\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5000\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    input_batch, target_batch, cooc_batch, weighting_batch = random_batch(batch_size, news_corpus, skip_grams, X_ik, weighting_dic)\n",
    "    input_batch  = torch.LongTensor(input_batch)         #[batch_size, 1]\n",
    "    target_batch = torch.LongTensor(target_batch)        #[batch_size, 1]\n",
    "    cooc_batch   = torch.FloatTensor(cooc_batch)         #[batch_size, 1]\n",
    "    weighting_batch = torch.FloatTensor(weighting_batch) #[batch_size, 1]\n",
    "\n",
    "    # to cuda\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    cooc_batch = cooc_batch.to(device)\n",
    "    weighting_batch = weighting_batch.to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = model_glove(input_batch, target_batch, cooc_batch, weighting_batch)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f\"Epoch: {epoch + 1} | cost: {loss:.6f} | time: {epoch_mins}m {epoch_secs}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model_glove.state_dict(), 'model/glove_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model using pickle\n",
    "pickle.dump(model_glove, open('model/glove.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed_glove(word):\n",
    "    id_tensor = torch.LongTensor([word2index[word]])\n",
    "    id_tensor = id_tensor.to(device)\n",
    "    v_embed = model_glove.embedding_center(id_tensor)\n",
    "    u_embed = model_glove.embedding_outside(id_tensor) \n",
    "    word_embed = (v_embed + u_embed) / 2 \n",
    "    x, y = word_embed[0][0].item(), word_embed[0][1].item()\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "government = get_embed_glove('government')\n",
    "officials = get_embed_glove('officials')\n",
    "administration = get_embed_glove('administration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "government vs officials: 0.9994\n",
      "government vs administration: 0.3368\n",
      "government vs government: 1.0000\n"
     ]
    }
   ],
   "source": [
    "print(f\"government vs officials: {cos_sim(government, officials):.4f}\")\n",
    "print(f\"government vs administration: {cos_sim(government, administration):.4f}\")\n",
    "print(f\"government vs government: {cos_sim(government, government):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove (Genism)\n",
    "Source credit: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#you have to put this file in some python/gensim directory; just run it and it will inform where to put....\n",
    "glove_file = datapath('glove.6B.100d.txt')\n",
    "model_genism = KeyedVectors.load_word2vec_format(glove_file, binary=False, no_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'king' and 'queen': 0.7508\n",
      "King - Man + Woman =  queen\n"
     ]
    }
   ],
   "source": [
    "# Example: Word similarity\n",
    "similarity = model_genism.similarity('king', 'queen')\n",
    "print(f\"Similarity between 'king' and 'queen': {similarity:.4f}\")\n",
    "\n",
    "# Example: Word analogy\n",
    "result = model_genism.most_similar(positive=['king', 'woman'], negative=['man'])\n",
    "print(\"King - Man + Woman = \", result[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_word(word1, word2, word3, embeddings, word_to_index, index_to_word):\n",
    "    # Get vectors for w1, w2, w3\n",
    "    vec1 = np.array(embeddings(word1))\n",
    "    vec2 = np.array(embeddings(word2))\n",
    "    vec3 = np.array(embeddings(word3))\n",
    "\n",
    "    # Calculate the predicted vector\n",
    "    predicted_vec = vec1 - vec2 + vec3\n",
    "\n",
    "    # Find the closest word by cosine similarity\n",
    "    max_similarity = -1\n",
    "    best_word = None\n",
    "    for word, index in word_to_index.items():\n",
    "        if word in [word1, word2, word3]:  # Skip the input words\n",
    "            continue\n",
    "        similarity = cos_sim(predicted_vec, embeddings(word))\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            best_word = word\n",
    "\n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate accuracy\n",
    "def evaluate_analogies(analogy_dataset, embeddings, word_to_index):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for analogy in analogy_dataset:\n",
    "        word1, word2, word3, word4 = analogy\n",
    "        if word1 not in word_to_index or word2 not in word_to_index or word3 not in word_to_index or word4 not in word_to_index:\n",
    "            continue  # Skip if any word is not in the vocabulary\n",
    "        predicted_word = predict_word(word1, word2, word3, embeddings, word_to_index, {v: k for k, v in word_to_index.items()})\n",
    "        if predicted_word == word4:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    return correct / total if total > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Source credit: https://www.fit.vut.cz/person/imikolov/public/rnnlm/word-test.v1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"capital-common-countries.txt\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "semantic_dataset = []\n",
    "for line in lines:\n",
    "    # Split the line into words\n",
    "    words = line.strip().split()\n",
    "    if len(words) == 4:\n",
    "        semantic_dataset.append([words[0], words[1], words[2], words[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"past-tense.txt\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "past_tense_dataset = []\n",
    "for line in lines:\n",
    "    words = line.strip().split()\n",
    "    if len(words) == 4:\n",
    "        past_tense_dataset.append([words[0], words[1], words[2], words[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Athens', 'Greece', 'Baghdad', 'Iraq'],\n",
       " ['Athens', 'Greece', 'Bangkok', 'Thailand'],\n",
       " ['Athens', 'Greece', 'Beijing', 'China'],\n",
       " ['Athens', 'Greece', 'Berlin', 'Germany'],\n",
       " ['Athens', 'Greece', 'Bern', 'Switzerland']]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick check\n",
    "semantic_dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dancing', 'danced', 'decreasing', 'decreased'],\n",
       " ['dancing', 'danced', 'describing', 'described'],\n",
       " ['dancing', 'danced', 'enhancing', 'enhanced'],\n",
       " ['dancing', 'danced', 'falling', 'fell'],\n",
       " ['dancing', 'danced', 'feeding', 'fed']]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_tense_dataset[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntactic Accuracy - skipgram: 0.00%\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_analogies(past_tense_dataset, get_embed_skip_gram, word2index)\n",
    "print(f\"Syntactic Accuracy - skipgram: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntactic Accuracy - negative sample: 0.00%\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_analogies(past_tense_dataset, get_embed_neg_sample, word2index)\n",
    "print(f\"Syntactic Accuracy - negative sample: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntactic Accuracy - glove: 0.00%\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_analogies(past_tense_dataset, get_embed_glove, word2index)\n",
    "print(f\"Syntactic Accuracy - glove: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntactic Accuracy - gensim: 55.45%\n"
     ]
    }
   ],
   "source": [
    "accuracy = model_genism.evaluate_word_analogies(\"past_tense_lines.txt\")[0]\n",
    "print(f\"Syntactic Accuracy - gensim: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Accuracy - skipgram: 0.00%\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_analogies(semantic_dataset, get_embed_skip_gram, word2index)\n",
    "print(f\"Semantic Accuracy - skipgram: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Accuracy - negative sample: 0.00%\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_analogies(semantic_dataset, get_embed_neg_sample, word2index)\n",
    "print(f\"Semantic Accuracy - negative sample: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Accuracy - glove: 0.00%\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_analogies(semantic_dataset, get_embed_glove, word2index)\n",
    "print(f\"Semantic Accuracy - glove: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Accuracy - gensim: 93.87%\n"
     ]
    }
   ],
   "source": [
    "accuracy = model_genism.evaluate_word_analogies(\"capital.txt\")[0]\n",
    "print(f\"Semantic Accuracy - gensim: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | Window Size | Training Loss | Training Time (sec) | Syntactic Accuracy (%) | Semantic Accuracy (%) |\n",
    "|----------|----------|----------|----------|----------|----------|\n",
    "| Skipgram    | 2     | 7.60     | 310     | 0.0     | 0.0   |\n",
    "| Skipgram (Neg)   | 2     | 1.89     | 171     | 0.0     | 0.0     |\n",
    "| Glove    | 2     | 2.33    | 34     | 0.0    | 0.0     |\n",
    "| Glove (Genism)    | -     | -     | -     | 55.45     | 93.87     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Test\n",
    "Source credit: http://alfonseca.org/eng/research/wordsim353.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Similarity Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>computer</td>\n",
       "      <td>keyboard</td>\n",
       "      <td>7.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jerusalem</td>\n",
       "      <td>Israel</td>\n",
       "      <td>8.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>planet</td>\n",
       "      <td>galaxy</td>\n",
       "      <td>8.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>canyon</td>\n",
       "      <td>landscape</td>\n",
       "      <td>7.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OPEC</td>\n",
       "      <td>country</td>\n",
       "      <td>5.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>rooster</td>\n",
       "      <td>voyage</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>noon</td>\n",
       "      <td>string</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>chord</td>\n",
       "      <td>smile</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>professor</td>\n",
       "      <td>cucumber</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>king</td>\n",
       "      <td>cabbage</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word 1     Word 2  Similarity Index\n",
       "0     computer   keyboard              7.62\n",
       "1    Jerusalem     Israel              8.46\n",
       "2       planet     galaxy              8.11\n",
       "3       canyon  landscape              7.53\n",
       "4         OPEC    country              5.63\n",
       "..         ...        ...               ...\n",
       "247    rooster     voyage              0.62\n",
       "248       noon     string              0.54\n",
       "249      chord      smile              0.54\n",
       "250  professor   cucumber              0.31\n",
       "251       king    cabbage              0.23\n",
       "\n",
       "[252 rows x 3 columns]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns = ['Word 1', 'Word 2', 'Similarity Index']\n",
    "\n",
    "df = pd.read_csv('wordsim_relatedness_goldstandard.txt', sep='\\t', header=None, names=columns)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    word_1 = row['Word 1']\n",
    "    word_2 = row['Word 2']\n",
    "\n",
    "    try:\n",
    "        embed_1_neg_samp    = get_embed_neg_sample(word_1)\n",
    "        embed_2_neg_samp    = get_embed_neg_sample(word_2)\n",
    "        embed_1_skip_gram   = get_embed_skip_gram(word_1)\n",
    "        embed_2_skip_gram   = get_embed_skip_gram(word_2)\n",
    "        embed_1_glove       = get_embed_glove(word_1)\n",
    "        embed_2_glove       = get_embed_glove(word_2)\n",
    "\n",
    "    except KeyError:\n",
    "        # Replacing missing embeddings with the embedding of '<UNK>'\n",
    "        embed_1_neg_samp    = get_embed_neg_sample('<UNK>')\n",
    "        embed_2_neg_samp    = get_embed_neg_sample('<UNK>')\n",
    "        embed_1_skip_gram   = get_embed_skip_gram('<UNK>')\n",
    "        embed_2_skip_gram   = get_embed_skip_gram('<UNK>')\n",
    "        embed_1_glove       = get_embed_glove('<UNK>')\n",
    "        embed_2_glove       = get_embed_glove('<UNK>')\n",
    "\n",
    "    # Computing dot product\n",
    "    df.at[index, 'dot_product_neg_samp'] = np.dot(embed_1_neg_samp, embed_2_neg_samp)\n",
    "    df.at[index, 'dot_product_skip_gram'] = np.dot(embed_1_skip_gram, embed_2_skip_gram)\n",
    "    df.at[index, 'dot_product_glove'] = np.dot(embed_1_glove, embed_2_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Similarity Index</th>\n",
       "      <th>dot_product_neg_samp</th>\n",
       "      <th>dot_product_skip_gram</th>\n",
       "      <th>dot_product_glove</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>computer</td>\n",
       "      <td>keyboard</td>\n",
       "      <td>7.62</td>\n",
       "      <td>0.942009</td>\n",
       "      <td>3.773402</td>\n",
       "      <td>0.143185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jerusalem</td>\n",
       "      <td>Israel</td>\n",
       "      <td>8.46</td>\n",
       "      <td>0.942009</td>\n",
       "      <td>3.773402</td>\n",
       "      <td>0.143185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>planet</td>\n",
       "      <td>galaxy</td>\n",
       "      <td>8.11</td>\n",
       "      <td>0.942009</td>\n",
       "      <td>3.773402</td>\n",
       "      <td>0.143185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>canyon</td>\n",
       "      <td>landscape</td>\n",
       "      <td>7.53</td>\n",
       "      <td>0.942009</td>\n",
       "      <td>3.773402</td>\n",
       "      <td>0.143185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OPEC</td>\n",
       "      <td>country</td>\n",
       "      <td>5.63</td>\n",
       "      <td>0.942009</td>\n",
       "      <td>3.773402</td>\n",
       "      <td>0.143185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>day</td>\n",
       "      <td>summer</td>\n",
       "      <td>3.94</td>\n",
       "      <td>0.123261</td>\n",
       "      <td>0.049107</td>\n",
       "      <td>-0.134245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>day</td>\n",
       "      <td>dawn</td>\n",
       "      <td>7.53</td>\n",
       "      <td>0.942009</td>\n",
       "      <td>3.773402</td>\n",
       "      <td>0.143185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>country</td>\n",
       "      <td>citizen</td>\n",
       "      <td>7.31</td>\n",
       "      <td>0.942009</td>\n",
       "      <td>3.773402</td>\n",
       "      <td>0.143185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>planet</td>\n",
       "      <td>people</td>\n",
       "      <td>5.75</td>\n",
       "      <td>0.942009</td>\n",
       "      <td>3.773402</td>\n",
       "      <td>0.143185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>environment</td>\n",
       "      <td>ecology</td>\n",
       "      <td>8.81</td>\n",
       "      <td>0.942009</td>\n",
       "      <td>3.773402</td>\n",
       "      <td>0.143185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word 1     Word 2  Similarity Index  dot_product_neg_samp  \\\n",
       "0     computer   keyboard              7.62              0.942009   \n",
       "1    Jerusalem     Israel              8.46              0.942009   \n",
       "2       planet     galaxy              8.11              0.942009   \n",
       "3       canyon  landscape              7.53              0.942009   \n",
       "4         OPEC    country              5.63              0.942009   \n",
       "5          day     summer              3.94              0.123261   \n",
       "6          day       dawn              7.53              0.942009   \n",
       "7      country    citizen              7.31              0.942009   \n",
       "8       planet     people              5.75              0.942009   \n",
       "9  environment    ecology              8.81              0.942009   \n",
       "\n",
       "   dot_product_skip_gram  dot_product_glove  \n",
       "0               3.773402           0.143185  \n",
       "1               3.773402           0.143185  \n",
       "2               3.773402           0.143185  \n",
       "3               3.773402           0.143185  \n",
       "4               3.773402           0.143185  \n",
       "5               0.049107          -0.134245  \n",
       "6               3.773402           0.143185  \n",
       "7               3.773402           0.143185  \n",
       "8               3.773402           0.143185  \n",
       "9               3.773402           0.143185  "
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Computing the Spearman correlation\n",
    "correlation_pos, _ = spearmanr(df['Similarity Index'], df['dot_product_skip_gram'])\n",
    "correlation_neg, _ = spearmanr(df['Similarity Index'], df['dot_product_neg_samp'])\n",
    "correlation_glove, _ = spearmanr(df['Similarity Index'], df['dot_product_glove'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Correlation Coefficient of Skipgram: 0.0110\n",
      "Spearman Correlation Coefficient of Skipgram with Negative Sampling: 0.0385\n",
      "Spearman Correlation Coefficient of Glove: -0.0356\n"
     ]
    }
   ],
   "source": [
    "print(f\"Spearman Correlation Coefficient of Skipgram: {correlation_pos:.4f}\")\n",
    "print(f\"Spearman Correlation Coefficient of Skipgram with Negative Sampling: {correlation_neg:.4f}\")\n",
    "print(f\"Spearman Correlation Coefficient of Glove: {correlation_glove:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true: 5.29\n"
     ]
    }
   ],
   "source": [
    "# Finding y_true based on the mean of similarity index in the df\n",
    "y_true = df['Similarity Index'].mean()\n",
    "\n",
    "print(f\"y_true: {y_true:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Correlation Correlation coefficient of Glove (genism): 0.50\n"
     ]
    }
   ],
   "source": [
    "correlation_coefficient = model_genism.evaluate_word_pairs('wordsim_relatedness_goldstandard.txt')\n",
    "print(f\"Spearman Correlation Correlation coefficient of Glove (genism): {correlation_coefficient[1][0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | Skipgram | NEG | GloVe | GloVe (genism) | Y_true |\n",
    "|----------|----------|----------|----------|----------|----------|\n",
    "| MSE    | 0.011     | 0.0385     | -0.03     | 0.5     | 5.29   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed_for_corpus(model, words):\n",
    "    embeddings = {}\n",
    "\n",
    "    for word in words:\n",
    "        try:\n",
    "            index = word2index[word]\n",
    "        except KeyError:\n",
    "            index = word2index['<UNK>']\n",
    "\n",
    "        word_tensor = torch.LongTensor([index])\n",
    "        word_tensor = word_tensor.to(device)\n",
    "\n",
    "        embed_c = model.embedding_center(word_tensor)\n",
    "        embed_o = model.embedding_outside(word_tensor)\n",
    "        embed = (embed_c + embed_o) / 2\n",
    "\n",
    "        # return as dictionary with key as the word and value as the array of its embedding\n",
    "        embeddings[word] = np.array([embed[0][0].item(), embed[0][1].item()])\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_whole_glove = get_embed_for_corpus(model_glove, vocab)\n",
    "embed_whole_neg_skg = get_embed_for_corpus(model_neg, vocab)\n",
    "embed_whole_skg = get_embed_for_corpus(model_glove, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model/model_gensim.pkl', 'wb') as model_file:\n",
    "    pickle.dump(model_genism, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved.\n"
     ]
    }
   ],
   "source": [
    "with open('model/embed_skipgram_negative.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(embed_whole_neg_skg, pickle_file)\n",
    "\n",
    "print(f\"File saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved.\n"
     ]
    }
   ],
   "source": [
    "with open('model/embed_skipgram.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(embed_whole_skg, pickle_file)\n",
    "\n",
    "print(f\"File saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved.\n"
     ]
    }
   ],
   "source": [
    "with open('model/embed_glove.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(embed_whole_glove, pickle_file)\n",
    "\n",
    "print(f\"File saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As observed, for window size of 2 - skipgram had the highest (7.60) training loss while Negative skipgram had the lowest (1.89). Glove also performed much better compared to skipgram with loss of 2.33.\n",
    "\n",
    "In case of Training time each model were trained for 5000 epoch. Skipgram took the longest time with 310s and Glove took the least amount of time with 34s. Negative sampling took 171s. \n",
    "\n",
    "All three models coded from scratch performed bad compared to Genism. This is because of the small corpus size and window size. All 3 models (Skipgram, Skipgram with negative sampling, Glove) had syantactic and semantic accuracy of 0%. This was expected because of the limitations of our corpus. Glove (Genism) on the other achieved Syntactic accuracy of 55.45% \n",
    "and Semantic Accuracy of 93.87%.\n",
    "\n",
    "Furthermore, for Spearman Correlation Coefficient - Genism outperforms other models with correlation score of `0.5`. The other 3 models showed poor correlation which suggests that predicted rankings do not closely match with ground truth. So our embeddings has poor correlation with human judgement.\n",
    "\n",
    "In conclusion, given the small corpus size, window size and embedding dimension - our 'made from scratch' models performed poorly. Given better hyperparameter tunings like embeddig dimensions, learning rate and even number of epochs, the models can be refined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ait",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
